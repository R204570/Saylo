# LOCAL MVP IMPLEMENTATION PLAN
## Hardware-Optimized AI Interview Platform

### ğŸ¯ MVP SCOPE
**Goal**: Single working interview session with AI voice interviewer

**What's Included**:
- âœ… One user account (no auth for MVP)
- âœ… One subject (e.g., "Software Engineer Interview")
- âœ… One resume upload & parsing
- âœ… One reference document (PDF)
- âœ… Real-time AI voice conversation
- âœ… Resume-aware question generation
- âœ… Context-aware assistance
- âœ… Basic proctoring (face detection)
- âœ… Session transcript
- âœ… Simple analytics report

**What's Excluded** (for now):
- âŒ Multi-user authentication
- âŒ Multiple subjects
- âŒ MCQ popups
- âŒ Advanced proctoring
- âŒ Complex analytics dashboard

---

## ğŸ—ï¸ ARCHITECTURE (LOCAL-FIRST)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (Simple HTML/JS)           â”‚
â”‚         localhost:8080                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Backend (localhost:8000)       â”‚
â”‚  - Session management                       â”‚
â”‚  - File upload/processing                   â”‚
â”‚  - API endpoints                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama     â”‚ â”‚  ChromaDB    â”‚ â”‚  PostgreSQL  â”‚
â”‚ localhost    â”‚ â”‚  (local)     â”‚ â”‚  (local)     â”‚
â”‚  :11434      â”‚ â”‚              â”‚ â”‚  :5432       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LiveKit (Self-Hosted)               â”‚
â”‚         localhost:7880                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  AI MODEL SELECTION (VRAM-OPTIMIZED)

### Primary Models (Total VRAM: ~3.5GB)
1. **LLM**: `llama3.1:8b-instruct-q4_K_M` (~4.5GB on disk, ~3GB VRAM)
   - Question generation
   - Answer evaluation
   - Context-aware responses

2. **Embeddings**: `nomic-embed-text` (CPU-based, ~300MB)
   - Document vectorization
   - Semantic search

3. **STT**: `whisper-small` (CPU-based, ~500MB)
   - Speech-to-text transcription
   - Runs on CPU to save VRAM

4. **TTS**: `piper-tts` (CPU-based, ~50MB)
   - Text-to-speech for AI voice
   - Fast, local, CPU-based

5. **Vision** (Optional): `llava:7b-q4` (~4GB)
   - Face detection for proctoring
   - **Strategy**: Run ONLY when LLM is idle
   - Sample 1 frame every 5 seconds

### VRAM Management Strategy
- **Sequential Processing**: Never run LLM + Vision simultaneously
- **Model Unloading**: Unload vision model after frame processing
- **CPU Offloading**: Use CPU for embeddings, STT, TTS
- **Batch Processing**: Process vision frames in batches during pauses

---

## ğŸ“ PROJECT STRUCTURE

```
Saylo/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”‚   â”œâ”€â”€ models/                 # SQLAlchemy models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py
â”‚   â”‚   â”‚   â”œâ”€â”€ transcript.py
â”‚   â”‚   â”‚   â””â”€â”€ analytics.py
â”‚   â”‚   â”œâ”€â”€ services/               # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_service.py   # Ollama integration
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_service.py   # ChromaDB operations
â”‚   â”‚   â”‚   â”œâ”€â”€ document_service.py # PDF processing
â”‚   â”‚   â”‚   â”œâ”€â”€ stt_service.py      # Whisper STT
â”‚   â”‚   â”‚   â”œâ”€â”€ tts_service.py      # Piper TTS
â”‚   â”‚   â”‚   â”œâ”€â”€ vision_service.py   # Face detection
â”‚   â”‚   â”‚   â””â”€â”€ livekit_service.py  # LiveKit integration
â”‚   â”‚   â”œâ”€â”€ api/                    # API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â”‚   â””â”€â”€ interview.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ pdf_parser.py
â”‚   â”‚       â””â”€â”€ resume_parser.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ app.js
â”‚   â”‚   â”œâ”€â”€ livekit-client.js
â”‚   â”‚   â””â”€â”€ interview.js
â”‚   â””â”€â”€ assets/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/                    # Resume & docs
â”‚   â”œâ”€â”€ chromadb/                   # Vector storage
â”‚   â””â”€â”€ recordings/                 # Session recordings
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_local.sh              # Linux/Mac setup
â”‚   â”œâ”€â”€ setup_local.ps1             # Windows setup
â”‚   â”œâ”€â”€ start_services.sh
â”‚   â””â”€â”€ start_services.ps1
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml          # All services
â”‚   â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ livekit/
â”œâ”€â”€ models/                         # Downloaded models
â”‚   â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ whisper/
â”‚   â””â”€â”€ piper/
â”œâ”€â”€ Developement.txt
â”œâ”€â”€ README.md
â””â”€â”€ LOCAL_MVP_PLAN.md
```

---

## ğŸ”§ SETUP SEQUENCE

### Step 1: Install Dependencies (Windows)
```powershell
# Python 3.11
# PostgreSQL 15
# Ollama
# FFmpeg (for audio processing)
```

### Step 2: Download Models
```bash
# Ollama models
ollama pull llama3.1:8b-instruct-q4_K_M
ollama pull nomic-embed-text

# Whisper (via faster-whisper)
# Auto-downloads on first use

# Piper TTS
# Auto-downloads on first use
```

### Step 3: Initialize Databases
```sql
-- PostgreSQL schema
-- ChromaDB auto-initializes
```

### Step 4: Start Services
```powershell
# Terminal 1: Ollama (if not running as service)
ollama serve

# Terminal 2: PostgreSQL (if not running as service)
# Usually runs as Windows service

# Terminal 3: Backend
cd backend
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Terminal 4: Frontend (simple HTTP server)
cd frontend
python -m http.server 8080

# Terminal 5: LiveKit (Docker)
docker-compose up livekit
```

---

## ğŸ¬ MVP WORKFLOW

### 1. **Setup Phase** (One-time)
```
User â†’ Upload Resume (PDF) â†’ Backend parses â†’ Store in DB
User â†’ Upload Reference Doc (PDF) â†’ Backend chunks â†’ Store in ChromaDB
```

### 2. **Interview Session Start**
```
User â†’ Click "Start Interview" â†’ Backend:
  - Create session record (PostgreSQL)
  - Create LiveKit room
  - Load resume context
  - Generate first question (Ollama)
  - Return room token to frontend
```

### 3. **Real-time Interview Loop**
```
Frontend connects to LiveKit room
â†“
AI Agent joins room (backend)
â†“
AI speaks first question (TTS â†’ LiveKit audio track)
â†“
User speaks answer â†’ LiveKit captures audio
â†“
Backend receives audio â†’ Whisper STT â†’ Text
â†“
Text â†’ Ollama (evaluate + generate next question)
â†“
Response â†’ TTS â†’ LiveKit audio
â†“
Repeat 5-10 questions
```

### 4. **Proctoring** (Background)
```
Every 5 seconds:
  - Capture 1 frame from user video
  - If LLM idle: Run face detection (llava)
  - Log anomalies (no face, multiple faces)
  - Store in DB
```

### 5. **Session End**
```
User â†’ Click "End Interview"
â†“
Backend:
  - Stop LiveKit room
  - Generate transcript (from stored text)
  - Calculate basic scores
  - Generate analytics report
  - Display to user
```

---

## ğŸ“Š ANALYTICS (MVP)

### Simple Metrics:
1. **Duration**: Total session time
2. **Questions Answered**: Count
3. **Average Response Time**: Per question
4. **Proctoring Flags**: Count of anomalies
5. **Transcript**: Full conversation text
6. **Key Topics**: Extracted from questions (using embeddings)

### Report Format (JSON):
```json
{
  "session_id": "uuid",
  "duration_seconds": 1200,
  "questions_count": 8,
  "avg_response_time": 45,
  "proctoring_flags": 2,
  "topics_covered": ["Python", "System Design", "Algorithms"],
  "transcript": [...],
  "recommendations": "Focus on system design concepts"
}
```

---

## âš¡ PERFORMANCE OPTIMIZATIONS

### 1. **VRAM Management**
- Load LLM on startup, keep in memory
- Unload vision model after each use
- Use CPU for embeddings/STT/TTS

### 2. **Latency Reduction**
- Pre-generate 2-3 questions ahead
- Cache common prompts
- Use streaming for LLM responses
- Async processing everywhere

### 3. **Context Window**
- Max 2000 tokens per prompt
- Include: Last 2 Q&A + Resume snippet + Retrieved docs (top 3)
- Aggressive summarization

### 4. **Frame Processing**
- 1 frame / 5 seconds (not every frame)
- Resize to 224x224 before vision model
- Skip if LLM busy

---

## ğŸš€ IMPLEMENTATION PHASES

### Phase 1: Core Backend (Day 1-2)
- [ ] FastAPI setup
- [ ] PostgreSQL schema
- [ ] Basic API endpoints
- [ ] File upload handling

### Phase 2: AI Integration (Day 3-4)
- [ ] Ollama service wrapper
- [ ] ChromaDB integration
- [ ] Document processing pipeline
- [ ] Resume parsing

### Phase 3: Real-time Features (Day 5-6)
- [ ] LiveKit integration
- [ ] STT service (Whisper)
- [ ] TTS service (Piper)
- [ ] Audio streaming

### Phase 4: Interview Logic (Day 7-8)
- [ ] Question generation
- [ ] Answer evaluation
- [ ] Conversation flow
- [ ] Session management

### Phase 5: Frontend (Day 9-10)
- [ ] Simple UI
- [ ] LiveKit client
- [ ] File upload interface
- [ ] Interview interface

### Phase 6: Proctoring (Day 11)
- [ ] Vision service
- [ ] Face detection
- [ ] Anomaly logging

### Phase 7: Analytics (Day 12)
- [ ] Transcript generation
- [ ] Score calculation
- [ ] Report generation

### Phase 8: Testing & Polish (Day 13-14)
- [ ] End-to-end testing
- [ ] Performance tuning
- [ ] Bug fixes
- [ ] Documentation

---

## ğŸ¯ SUCCESS CRITERIA

### MVP is successful if:
1. âœ… User can upload resume + reference doc
2. âœ… AI generates relevant questions based on resume
3. âœ… Real-time voice conversation works (< 5s latency)
4. âœ… Transcription is accurate (> 85%)
5. âœ… Basic face detection works
6. âœ… Session completes without crashes
7. âœ… Analytics report is generated
8. âœ… VRAM stays under 3.8GB
9. âœ… System runs smoothly on target hardware

---

## ğŸ”„ NEXT STEPS AFTER MVP

Once MVP is stable:
1. Add MCQ popup system
2. Improve proctoring (gaze detection, tab switching)
3. Add multiple subjects
4. Implement user authentication
5. Enhanced analytics dashboard
6. Question difficulty calibration
7. Multi-language support
8. Mobile app

---

## ğŸ“ NOTES

- **No Cloud Dependencies**: Everything runs locally
- **Internet Only For**: LiveKit signaling (can be self-hosted), model downloads
- **Fallbacks**: If vision model too slow, disable proctoring
- **Debugging**: Extensive logging for troubleshooting
- **Scalability**: Architecture supports future cloud deployment

---

**Let's build this step by step! ğŸš€**
